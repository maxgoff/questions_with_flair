{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, flair\n",
    "torch.cuda.is_available()\n",
    "flair.device\n",
    "flair.device = torch.device('cuda:0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-15 02:30:38,176 Reading data from /home/dmax/.flair/datasets/trec_6\n",
      "2021-07-15 02:30:38,177 Train: /home/dmax/.flair/datasets/trec_6/train.txt\n",
      "2021-07-15 02:30:38,178 Dev: None\n",
      "2021-07-15 02:30:38,178 Test: /home/dmax/.flair/datasets/trec_6/test.txt\n",
      "2021-07-15 02:30:38,620 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5472/5472 [00:00<00:00, 147306.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-15 02:30:38,661 [b'open question', b'closed question']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-15 02:30:48,018 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:30:48,020 Model: \"TARSClassifier(\n",
      "  (document_embeddings): None\n",
      "  (decoder): None\n",
      "  (loss_function): None\n",
      "  (tars_model): TextClassifier(\n",
      "    (document_embeddings): TransformerDocumentEmbeddings(\n",
      "      (model): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (loss_function): CrossEntropyLoss()\n",
      "  )\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-15 02:30:48,021 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:30:48,021 Corpus: \"Corpus: 4972 train + 552 dev + 500 test sentences\"\n",
      "2021-07-15 02:30:48,022 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:30:48,022 Parameters:\n",
      "2021-07-15 02:30:48,023  - learning_rate: \"0.02\"\n",
      "2021-07-15 02:30:48,024  - mini_batch_size: \"32\"\n",
      "2021-07-15 02:30:48,024  - patience: \"3\"\n",
      "2021-07-15 02:30:48,025  - anneal_factor: \"0.5\"\n",
      "2021-07-15 02:30:48,025  - max_epochs: \"10\"\n",
      "2021-07-15 02:30:48,026  - shuffle: \"True\"\n",
      "2021-07-15 02:30:48,026  - train_with_dev: \"False\"\n",
      "2021-07-15 02:30:48,027  - batch_growth_annealing: \"False\"\n",
      "2021-07-15 02:30:48,027 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:30:48,028 Model training base path: \"resources/taggers/trec\"\n",
      "2021-07-15 02:30:48,028 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:30:48,029 Device: cuda:0\n",
      "2021-07-15 02:30:48,030 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:30:48,030 Embeddings storage mode: cpu\n",
      "2021-07-15 02:30:48,033 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:30:56,191 epoch 1 - iter 15/156 - loss 0.83465401 - samples/sec: 59.06 - lr: 0.020000\n",
      "2021-07-15 02:31:04,433 epoch 1 - iter 30/156 - loss 0.73572363 - samples/sec: 58.24 - lr: 0.020000\n",
      "2021-07-15 02:31:12,546 epoch 1 - iter 45/156 - loss 0.66079240 - samples/sec: 59.18 - lr: 0.020000\n",
      "2021-07-15 02:31:20,731 epoch 1 - iter 60/156 - loss 0.60212494 - samples/sec: 58.66 - lr: 0.020000\n",
      "2021-07-15 02:31:28,897 epoch 1 - iter 75/156 - loss 0.56837040 - samples/sec: 58.79 - lr: 0.020000\n",
      "2021-07-15 02:31:37,025 epoch 1 - iter 90/156 - loss 0.52923233 - samples/sec: 59.07 - lr: 0.020000\n",
      "2021-07-15 02:31:45,357 epoch 1 - iter 105/156 - loss 0.50232515 - samples/sec: 57.62 - lr: 0.020000\n",
      "2021-07-15 02:31:53,426 epoch 1 - iter 120/156 - loss 0.47843616 - samples/sec: 59.49 - lr: 0.020000\n",
      "2021-07-15 02:32:01,449 epoch 1 - iter 135/156 - loss 0.45666659 - samples/sec: 59.84 - lr: 0.020000\n",
      "2021-07-15 02:32:09,608 epoch 1 - iter 150/156 - loss 0.44140203 - samples/sec: 58.84 - lr: 0.020000\n",
      "2021-07-15 02:32:12,445 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:32:12,446 EPOCH 1 done: loss 0.4584 - lr 0.0200000\n",
      "2021-07-15 02:32:19,911 DEV : loss 0.36704206466674805 - score 0.8442\n",
      "2021-07-15 02:32:19,916 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2021-07-15 02:32:20,537 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:32:28,642 epoch 2 - iter 15/156 - loss 0.24758688 - samples/sec: 59.42 - lr: 0.020000\n",
      "2021-07-15 02:32:36,807 epoch 2 - iter 30/156 - loss 0.22927334 - samples/sec: 58.80 - lr: 0.020000\n",
      "2021-07-15 02:32:44,894 epoch 2 - iter 45/156 - loss 0.21851044 - samples/sec: 59.37 - lr: 0.020000\n",
      "2021-07-15 02:32:53,198 epoch 2 - iter 60/156 - loss 0.21124810 - samples/sec: 57.81 - lr: 0.020000\n",
      "2021-07-15 02:33:01,317 epoch 2 - iter 75/156 - loss 0.20423328 - samples/sec: 59.14 - lr: 0.020000\n",
      "2021-07-15 02:33:09,454 epoch 2 - iter 90/156 - loss 0.19974948 - samples/sec: 59.00 - lr: 0.020000\n",
      "2021-07-15 02:33:17,442 epoch 2 - iter 105/156 - loss 0.19253043 - samples/sec: 60.10 - lr: 0.020000\n",
      "2021-07-15 02:33:25,580 epoch 2 - iter 120/156 - loss 0.18747150 - samples/sec: 59.00 - lr: 0.020000\n",
      "2021-07-15 02:33:33,698 epoch 2 - iter 135/156 - loss 0.18147722 - samples/sec: 59.14 - lr: 0.020000\n",
      "2021-07-15 02:33:41,768 epoch 2 - iter 150/156 - loss 0.18364046 - samples/sec: 59.49 - lr: 0.020000\n",
      "2021-07-15 02:33:44,699 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:33:44,700 EPOCH 2 done: loss 0.1811 - lr 0.0200000\n",
      "2021-07-15 02:33:52,165 DEV : loss 0.22473838925361633 - score 0.9493\n",
      "2021-07-15 02:33:52,170 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2021-07-15 02:33:52,785 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:34:01,001 epoch 3 - iter 15/156 - loss 0.11462841 - samples/sec: 58.59 - lr: 0.020000\n",
      "2021-07-15 02:34:09,033 epoch 3 - iter 30/156 - loss 0.11533923 - samples/sec: 59.77 - lr: 0.020000\n",
      "2021-07-15 02:34:17,155 epoch 3 - iter 45/156 - loss 0.11433329 - samples/sec: 59.12 - lr: 0.020000\n",
      "2021-07-15 02:34:25,312 epoch 3 - iter 60/156 - loss 0.11579496 - samples/sec: 58.86 - lr: 0.020000\n",
      "2021-07-15 02:34:33,473 epoch 3 - iter 75/156 - loss 0.11284596 - samples/sec: 58.83 - lr: 0.020000\n",
      "2021-07-15 02:34:41,525 epoch 3 - iter 90/156 - loss 0.11484152 - samples/sec: 59.62 - lr: 0.020000\n",
      "2021-07-15 02:34:49,668 epoch 3 - iter 105/156 - loss 0.11277512 - samples/sec: 58.96 - lr: 0.020000\n",
      "2021-07-15 02:34:57,772 epoch 3 - iter 120/156 - loss 0.10982133 - samples/sec: 59.24 - lr: 0.020000\n",
      "2021-07-15 02:35:06,151 epoch 3 - iter 135/156 - loss 0.11377113 - samples/sec: 57.30 - lr: 0.020000\n",
      "2021-07-15 02:35:14,312 epoch 3 - iter 150/156 - loss 0.11303185 - samples/sec: 58.83 - lr: 0.020000\n",
      "2021-07-15 02:35:17,230 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:35:17,231 EPOCH 3 done: loss 0.1109 - lr 0.0200000\n",
      "2021-07-15 02:35:24,702 DEV : loss 0.24159403145313263 - score 0.9438\n",
      "2021-07-15 02:35:24,707 BAD EPOCHS (no improvement): 1\n",
      "2021-07-15 02:35:24,708 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:35:32,905 epoch 4 - iter 15/156 - loss 0.06006619 - samples/sec: 58.72 - lr: 0.020000\n",
      "2021-07-15 02:35:41,030 epoch 4 - iter 30/156 - loss 0.09146043 - samples/sec: 59.09 - lr: 0.020000\n",
      "2021-07-15 02:35:49,176 epoch 4 - iter 45/156 - loss 0.07992184 - samples/sec: 58.94 - lr: 0.020000\n",
      "2021-07-15 02:35:57,285 epoch 4 - iter 60/156 - loss 0.07926305 - samples/sec: 59.21 - lr: 0.020000\n",
      "2021-07-15 02:36:05,502 epoch 4 - iter 75/156 - loss 0.08075334 - samples/sec: 58.43 - lr: 0.020000\n",
      "2021-07-15 02:36:13,587 epoch 4 - iter 90/156 - loss 0.07897286 - samples/sec: 59.39 - lr: 0.020000\n",
      "2021-07-15 02:36:21,666 epoch 4 - iter 105/156 - loss 0.07352513 - samples/sec: 59.43 - lr: 0.020000\n",
      "2021-07-15 02:36:29,784 epoch 4 - iter 120/156 - loss 0.07403442 - samples/sec: 59.14 - lr: 0.020000\n",
      "2021-07-15 02:36:37,841 epoch 4 - iter 135/156 - loss 0.07724502 - samples/sec: 59.59 - lr: 0.020000\n",
      "2021-07-15 02:36:45,898 epoch 4 - iter 150/156 - loss 0.07764466 - samples/sec: 59.59 - lr: 0.020000\n",
      "2021-07-15 02:36:48,819 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:36:48,820 EPOCH 4 done: loss 0.0769 - lr 0.0200000\n",
      "2021-07-15 02:36:56,283 DEV : loss 0.22335201501846313 - score 0.9529\n",
      "2021-07-15 02:36:56,288 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2021-07-15 02:36:56,902 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:37:04,956 epoch 5 - iter 15/156 - loss 0.06466936 - samples/sec: 59.77 - lr: 0.020000\n",
      "2021-07-15 02:37:13,309 epoch 5 - iter 30/156 - loss 0.05785694 - samples/sec: 57.47 - lr: 0.020000\n",
      "2021-07-15 02:37:21,344 epoch 5 - iter 45/156 - loss 0.05619435 - samples/sec: 59.76 - lr: 0.020000\n",
      "2021-07-15 02:37:29,512 epoch 5 - iter 60/156 - loss 0.05268201 - samples/sec: 58.77 - lr: 0.020000\n",
      "2021-07-15 02:37:37,702 epoch 5 - iter 75/156 - loss 0.05038817 - samples/sec: 58.63 - lr: 0.020000\n",
      "2021-07-15 02:37:45,820 epoch 5 - iter 90/156 - loss 0.05022030 - samples/sec: 59.14 - lr: 0.020000\n",
      "2021-07-15 02:37:53,918 epoch 5 - iter 105/156 - loss 0.05115314 - samples/sec: 59.28 - lr: 0.020000\n",
      "2021-07-15 02:38:02,028 epoch 5 - iter 120/156 - loss 0.05181408 - samples/sec: 59.20 - lr: 0.020000\n",
      "2021-07-15 02:38:10,240 epoch 5 - iter 135/156 - loss 0.05244908 - samples/sec: 58.47 - lr: 0.020000\n",
      "2021-07-15 02:38:18,285 epoch 5 - iter 150/156 - loss 0.05428808 - samples/sec: 59.68 - lr: 0.020000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-15 02:38:21,173 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:38:21,174 EPOCH 5 done: loss 0.0533 - lr 0.0200000\n",
      "2021-07-15 02:38:28,625 DEV : loss 0.2764432728290558 - score 0.9601\n",
      "2021-07-15 02:38:28,630 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2021-07-15 02:38:29,243 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:38:37,281 epoch 6 - iter 15/156 - loss 0.04341389 - samples/sec: 59.89 - lr: 0.020000\n",
      "2021-07-15 02:38:45,495 epoch 6 - iter 30/156 - loss 0.04142258 - samples/sec: 58.45 - lr: 0.020000\n",
      "2021-07-15 02:38:53,630 epoch 6 - iter 45/156 - loss 0.04223469 - samples/sec: 59.01 - lr: 0.020000\n",
      "2021-07-15 02:39:01,602 epoch 6 - iter 60/156 - loss 0.04154789 - samples/sec: 60.22 - lr: 0.020000\n",
      "2021-07-15 02:39:09,909 epoch 6 - iter 75/156 - loss 0.03613127 - samples/sec: 57.79 - lr: 0.020000\n",
      "2021-07-15 02:39:18,047 epoch 6 - iter 90/156 - loss 0.04014152 - samples/sec: 59.00 - lr: 0.020000\n",
      "2021-07-15 02:39:26,202 epoch 6 - iter 105/156 - loss 0.03720413 - samples/sec: 58.87 - lr: 0.020000\n",
      "2021-07-15 02:39:34,253 epoch 6 - iter 120/156 - loss 0.03568168 - samples/sec: 59.63 - lr: 0.020000\n",
      "2021-07-15 02:39:42,325 epoch 6 - iter 135/156 - loss 0.03564679 - samples/sec: 59.48 - lr: 0.020000\n",
      "2021-07-15 02:39:50,349 epoch 6 - iter 150/156 - loss 0.03554208 - samples/sec: 59.84 - lr: 0.020000\n",
      "2021-07-15 02:39:53,224 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:39:53,225 EPOCH 6 done: loss 0.0358 - lr 0.0200000\n",
      "2021-07-15 02:40:00,686 DEV : loss 0.2982725203037262 - score 0.9293\n",
      "2021-07-15 02:40:00,692 BAD EPOCHS (no improvement): 1\n",
      "2021-07-15 02:40:00,693 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:40:08,792 epoch 7 - iter 15/156 - loss 0.03960774 - samples/sec: 59.43 - lr: 0.020000\n",
      "2021-07-15 02:40:17,074 epoch 7 - iter 30/156 - loss 0.02398241 - samples/sec: 57.97 - lr: 0.020000\n",
      "2021-07-15 02:40:25,173 epoch 7 - iter 45/156 - loss 0.02520578 - samples/sec: 59.28 - lr: 0.020000\n",
      "2021-07-15 02:40:33,334 epoch 7 - iter 60/156 - loss 0.02235076 - samples/sec: 58.83 - lr: 0.020000\n",
      "2021-07-15 02:40:41,526 epoch 7 - iter 75/156 - loss 0.02384438 - samples/sec: 58.61 - lr: 0.020000\n",
      "2021-07-15 02:40:49,589 epoch 7 - iter 90/156 - loss 0.02611699 - samples/sec: 59.54 - lr: 0.020000\n",
      "2021-07-15 02:40:57,621 epoch 7 - iter 105/156 - loss 0.02503593 - samples/sec: 59.78 - lr: 0.020000\n",
      "2021-07-15 02:41:05,697 epoch 7 - iter 120/156 - loss 0.02331819 - samples/sec: 59.45 - lr: 0.020000\n",
      "2021-07-15 02:41:13,786 epoch 7 - iter 135/156 - loss 0.02453742 - samples/sec: 59.35 - lr: 0.020000\n",
      "2021-07-15 02:41:21,922 epoch 7 - iter 150/156 - loss 0.02489337 - samples/sec: 59.01 - lr: 0.020000\n",
      "2021-07-15 02:41:24,804 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:41:24,805 EPOCH 7 done: loss 0.0241 - lr 0.0200000\n",
      "2021-07-15 02:41:32,500 DEV : loss 0.31833091378211975 - score 0.9493\n",
      "2021-07-15 02:41:32,505 BAD EPOCHS (no improvement): 2\n",
      "2021-07-15 02:41:32,507 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:41:40,722 epoch 8 - iter 15/156 - loss 0.01178577 - samples/sec: 58.59 - lr: 0.020000\n",
      "2021-07-15 02:41:48,808 epoch 8 - iter 30/156 - loss 0.02300554 - samples/sec: 59.38 - lr: 0.020000\n",
      "2021-07-15 02:41:56,886 epoch 8 - iter 45/156 - loss 0.02716328 - samples/sec: 59.43 - lr: 0.020000\n",
      "2021-07-15 02:42:05,025 epoch 8 - iter 60/156 - loss 0.02510520 - samples/sec: 58.99 - lr: 0.020000\n",
      "2021-07-15 02:42:13,031 epoch 8 - iter 75/156 - loss 0.02533429 - samples/sec: 59.97 - lr: 0.020000\n",
      "2021-07-15 02:42:21,178 epoch 8 - iter 90/156 - loss 0.02485729 - samples/sec: 58.93 - lr: 0.020000\n",
      "2021-07-15 02:42:29,291 epoch 8 - iter 105/156 - loss 0.02356198 - samples/sec: 59.18 - lr: 0.020000\n",
      "2021-07-15 02:42:37,620 epoch 8 - iter 120/156 - loss 0.02106015 - samples/sec: 57.64 - lr: 0.020000\n",
      "2021-07-15 02:42:45,677 epoch 8 - iter 135/156 - loss 0.02149752 - samples/sec: 59.59 - lr: 0.020000\n",
      "2021-07-15 02:42:53,765 epoch 8 - iter 150/156 - loss 0.02130530 - samples/sec: 59.35 - lr: 0.020000\n",
      "2021-07-15 02:42:56,652 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:42:56,653 EPOCH 8 done: loss 0.0210 - lr 0.0200000\n",
      "2021-07-15 02:43:04,117 DEV : loss 0.3129277527332306 - score 0.9601\n",
      "2021-07-15 02:43:04,123 BAD EPOCHS (no improvement): 3\n",
      "2021-07-15 02:43:04,124 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:43:12,228 epoch 9 - iter 15/156 - loss 0.00591957 - samples/sec: 59.40 - lr: 0.020000\n",
      "2021-07-15 02:43:20,247 epoch 9 - iter 30/156 - loss 0.00925448 - samples/sec: 59.87 - lr: 0.020000\n",
      "2021-07-15 02:43:28,313 epoch 9 - iter 45/156 - loss 0.01113670 - samples/sec: 59.52 - lr: 0.020000\n",
      "2021-07-15 02:43:36,676 epoch 9 - iter 60/156 - loss 0.01002862 - samples/sec: 57.41 - lr: 0.020000\n",
      "2021-07-15 02:43:44,810 epoch 9 - iter 75/156 - loss 0.01205927 - samples/sec: 59.03 - lr: 0.020000\n",
      "2021-07-15 02:43:52,906 epoch 9 - iter 90/156 - loss 0.01119510 - samples/sec: 59.31 - lr: 0.020000\n",
      "2021-07-15 02:44:00,936 epoch 9 - iter 105/156 - loss 0.01169011 - samples/sec: 59.79 - lr: 0.020000\n",
      "2021-07-15 02:44:09,014 epoch 9 - iter 120/156 - loss 0.01315406 - samples/sec: 59.44 - lr: 0.020000\n",
      "2021-07-15 02:44:17,126 epoch 9 - iter 135/156 - loss 0.01474446 - samples/sec: 59.18 - lr: 0.020000\n",
      "2021-07-15 02:44:25,333 epoch 9 - iter 150/156 - loss 0.01433928 - samples/sec: 58.50 - lr: 0.020000\n",
      "2021-07-15 02:44:28,253 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:44:28,254 EPOCH 9 done: loss 0.0149 - lr 0.0200000\n",
      "2021-07-15 02:44:35,911 DEV : loss 0.28743547201156616 - score 0.9583\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "2021-07-15 02:44:35,916 BAD EPOCHS (no improvement): 4\n",
      "2021-07-15 02:44:35,917 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:44:44,096 epoch 10 - iter 15/156 - loss 0.00381077 - samples/sec: 58.84 - lr: 0.010000\n",
      "2021-07-15 02:44:52,265 epoch 10 - iter 30/156 - loss 0.00293184 - samples/sec: 58.77 - lr: 0.010000\n",
      "2021-07-15 02:45:00,384 epoch 10 - iter 45/156 - loss 0.00607717 - samples/sec: 59.14 - lr: 0.010000\n",
      "2021-07-15 02:45:08,484 epoch 10 - iter 60/156 - loss 0.00716062 - samples/sec: 59.27 - lr: 0.010000\n",
      "2021-07-15 02:45:16,500 epoch 10 - iter 75/156 - loss 0.00792981 - samples/sec: 59.89 - lr: 0.010000\n",
      "2021-07-15 02:45:24,600 epoch 10 - iter 90/156 - loss 0.00730901 - samples/sec: 59.27 - lr: 0.010000\n",
      "2021-07-15 02:45:32,625 epoch 10 - iter 105/156 - loss 0.00689447 - samples/sec: 59.82 - lr: 0.010000\n",
      "2021-07-15 02:45:40,730 epoch 10 - iter 120/156 - loss 0.00681316 - samples/sec: 59.23 - lr: 0.010000\n",
      "2021-07-15 02:45:49,065 epoch 10 - iter 135/156 - loss 0.00805346 - samples/sec: 57.61 - lr: 0.010000\n",
      "2021-07-15 02:45:57,099 epoch 10 - iter 150/156 - loss 0.00765986 - samples/sec: 59.75 - lr: 0.010000\n",
      "2021-07-15 02:46:00,019 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:46:00,020 EPOCH 10 done: loss 0.0074 - lr 0.0100000\n",
      "2021-07-15 02:46:07,512 DEV : loss 0.30789679288864136 - score 0.9583\n",
      "2021-07-15 02:46:07,518 BAD EPOCHS (no improvement): 1\n",
      "2021-07-15 02:46:08,104 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-15 02:46:08,106 Testing using best model ...\n",
      "2021-07-15 02:46:08,107 loading file resources/taggers/trec/best-model.pt\n",
      "init TARS\n",
      "2021-07-15 02:46:19,844 \t0.982\n",
      "2021-07-15 02:46:19,845 \n",
      "Results:\n",
      "- F-score (micro) 0.982\n",
      "- F-score (macro) 0.9776\n",
      "- Accuracy 0.982\n",
      "\n",
      "By class:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  open question     0.9574    0.9783    0.9677       138\n",
      "closed question     0.9916    0.9834    0.9875       362\n",
      "\n",
      "      micro avg     0.9820    0.9820    0.9820       500\n",
      "      macro avg     0.9745    0.9808    0.9776       500\n",
      "   weighted avg     0.9822    0.9820    0.9821       500\n",
      "    samples avg     0.9820    0.9820    0.9820       500\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-15 02:46:19,846 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.982,\n",
       " 'dev_score_history': [0.8442,\n",
       "  0.9493,\n",
       "  0.9438,\n",
       "  0.9529,\n",
       "  0.9601,\n",
       "  0.9293,\n",
       "  0.9493,\n",
       "  0.9601,\n",
       "  0.9583,\n",
       "  0.9583],\n",
       " 'train_loss_history': [0.4583712617078653,\n",
       "  0.18114120808119574,\n",
       "  0.11091871216343954,\n",
       "  0.07689603697210072,\n",
       "  0.05325754987857997,\n",
       "  0.035774129757042736,\n",
       "  0.02409850013627408,\n",
       "  0.02097634753236213,\n",
       "  0.01493085554186389,\n",
       "  0.007394152739251671],\n",
       " 'dev_loss_history': [0.36704206466674805,\n",
       "  0.22473838925361633,\n",
       "  0.24159403145313263,\n",
       "  0.22335201501846313,\n",
       "  0.2764432728290558,\n",
       "  0.2982725203037262,\n",
       "  0.31833091378211975,\n",
       "  0.3129277527332306,\n",
       "  0.28743547201156616,\n",
       "  0.30789679288864136]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import flair\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import TREC_6\n",
    "from flair.models.text_classification_model import TARSClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "\n",
    "# 1. define label names as either OPEN or CLOSED \n",
    "\n",
    "label_name_map = {'CLOSED' : 'closed question',\n",
    "                  'OPEN'   : 'open question',\n",
    "                 }\n",
    "# 2. get the corpus   -- NOTE: data modified from original \n",
    "\n",
    "corpus: Corpus = TREC_6(label_name_map=label_name_map)\n",
    "\n",
    "# 3. create a TARS classifier\n",
    "tars = TARSClassifier(task_name='TREC_6', label_dictionary=corpus.make_label_dictionary())\n",
    "\n",
    "# 4. initialize the text classifier trainer\n",
    "trainer = ModelTrainer(tars, corpus)\n",
    "\n",
    "# 5. start the training\n",
    "trainer.train(base_path='resources/taggers/trec', # path to store the model artifacts\n",
    "              learning_rate=0.02, # use very small learning rate\n",
    "              mini_batch_size=32, #16,\n",
    "              #mini_batch_chunk_size=4, # optionally set this if transformer is too much for your machine\n",
    "              max_epochs=10, # terminate after 10 epochs\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Would you like vanilla ice cream ?\"   [− Tokens: 7  − Sentence-Labels: {'label': [closed question (0.9976)]}]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "sentence = Sentence(\"Would you like vanilla ice cream?\")\n",
    "\n",
    "# 3. Predict for food and drink\n",
    "tars.predict(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "===============\n",
      "\n",
      "Success\n",
      "Fail: Sentence: \"What are you planning to buy today at the supermarket ?\"   [− Tokens: 11  − Sentence-Labels: {'label': [closed question (0.9988)]}]\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Close: 1.0  Open: 0.9565217391304348\n"
     ]
    }
   ],
   "source": [
    "closed_sentences = [\"Are you feeling better today?\", \"May I use the bathroom?\", \"Is the prime rib a special tonight?\", \"Should I date him?\", \"Will you please do me a favor?\", \"Have you already completed your homework?\", \"Is that your final answer?\", \"Were you planning on becoming a fireman?\", \"Should I call her and sort things out?\", \"Is it wrong to want to live on my own at this age?\", \"Shall we make dinner together tonight?\", \"Could I possibly be a messier house guest?\", \"Might I be of service to you ladies this evening?\", \"Did that man walk by the house before?\", \"Can I help you with that?\", \"May I please have a bite of that pie?\", \"Would you like to go to the movies tonight?\", \"Is math your favorite subject?\", \"Does four plus four equal eight?\", \"Is that haunted house really scary?\", \"Will you be going to Grandmother's house for Christmas?\", \"Did Dad make the cake today?\", \"Is there a Mass being held at noon?\", \"Are you pregnant?\", \"Is he dead?\"] \n",
    "ccount = 0\n",
    "cscount = 0\n",
    "for sentence in closed_sentences: \n",
    "    s = Sentence(sentence) \n",
    "    tars.predict(s)\n",
    "    ccount += 1\n",
    "    if str(s.labels[0]).find(\"open\") > -1:\n",
    "        print(\"Fail:\",s)\n",
    "    else:\n",
    "        cscount += 1\n",
    "        print(\"Success\")\n",
    "print(\"===============\\n\")\n",
    "open_sentences = [\"What were the most important wars fought in the history of the United States?\", \"What are you planning to buy today at the supermarket?\", \"How exactly did the fight between the two of you start?\", \"What is your favorite memory from childhood?\", \"How will you help the company if you are hired to work for us?\", \"What do you plan to do immediately following graduation from college?\", \"What types of decorations do you plan to have for your friend's birthday party?\", \"What was your high school experience like?\", \"How did you and your best friend meet?\", \"What sights do you expect to see on your vacation?\", \"How do you go about booking tickets for a flight?\", \"What were the major effects of World War II for the United States?\", \"How do you go about purchasing a home?\", \"What is it like to live in Morocco?\", \"What is the quickest way to get to the pet store in town?\", \"Why is it that every time I talk with you, you seem irritated?\", \"How could I present myself better?\", \"How do you manage to raise those children alone?\", \"What is the matter with the people in that class?\", \"How are you going to find the time to write all those letters?\", \"Why can't I come along with you?\", \"What makes the leaves change color?\", \"How exactly does one replace the screen to a cellular phone?\"] \n",
    "\n",
    "ocount = 0\n",
    "oscount = 0\n",
    "for sentence in open_sentences: \n",
    "    s = Sentence(sentence) \n",
    "    tars.predict(s) \n",
    "    ocount += 1\n",
    "    if str(s.labels[0]).find(\"open\") > -1:\n",
    "        oscount += 1\n",
    "        print(\"Success\")\n",
    "    else:\n",
    "        print(\"Fail:\",s)\n",
    "        \n",
    "print(\"Closed:\", cscount/ccount, \" Open:\", oscount/ocount)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
